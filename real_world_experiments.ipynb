{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76cda3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximize reproducibility: set seed with minimal imports\n",
    "# just a seed\n",
    "seed = 431136\n",
    "import os\n",
    "\n",
    "# verbosity\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "# reproducibility\n",
    "# https://github.com/NVIDIA/framework-determinism\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(seed)\n",
    "rng_r = random.Random(seed + 1)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(seed + 2)\n",
    "rng_np = np.random.default_rng(seed + 3)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(seed + 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a8c6c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# from posthoceval.explainers import KernelSHAPExplainer\n",
    "# from posthoceval.explainers import MAPLEExplainer\n",
    "from posthoceval.explainers import *\n",
    "\n",
    "from posthoceval.models.gam import MultiClassLogisticGAM\n",
    "from posthoceval.models.gam import LinearGAM\n",
    "from posthoceval.models.gam import T\n",
    "from posthoceval.models.dnn import AdditiveDNN\n",
    "from posthoceval.models.cnn import AdditiveCNN\n",
    "from posthoceval.transform import Transformer\n",
    "from posthoceval.utils import nonexistent_filename\n",
    "from posthoceval.datasets import COMPASDataset\n",
    "from posthoceval.datasets import BostonDataset\n",
    "from posthoceval.datasets import HELOCDataset\n",
    "from posthoceval.datasets import TinyMNISTDataset\n",
    "from posthoceval.models.term_util import generate_terms\n",
    "from posthoceval.viz import gather_viz_data\n",
    "from posthoceval import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84906cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(\n",
    "    # context='paper',\n",
    "    context='notebook',\n",
    "    style='ticks',\n",
    "    font_scale=1,  # 2.25,\n",
    "    color_codes=True,\n",
    "    # palette=sns.color_palette('pastel'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32852243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpl_backend(backend='inline'):\n",
    "    rc = plt.rcParams.copy()\n",
    "    # backend = plt.get_backend()\n",
    "    %matplotlib $backend\n",
    "    \n",
    "mpl_inline = lambda: mpl_backend('inline')\n",
    "mpl_qt = lambda: mpl_backend('qt')\n",
    "mpl_notebook = lambda: mpl_backend('notebook')\n",
    "mpl_backend('nbagg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00b31a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples     = 27918\n",
      "Input Shape = (12, 10, 1)\n",
      "Task        = classification\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'mnist'\n",
    "\n",
    "if dataset_name == 'synthetic':\n",
    "    raise NotImplementedError\n",
    "    task = 'regression'\n",
    "\n",
    "    # import numpy\n",
    "    # X = np.random.rand(1000, 8) / 4\n",
    "    # x1, x2, x3, x4, x5, x6, x7, x8 = X.T\n",
    "    # y = (x1 ** 2 + x5 ** 2 + x5 * numpy.log(x1 + x2) +\n",
    "    #      x7 * numpy.select([numpy.greater(x2, numpy.sinc(x1 / numpy.pi)),\n",
    "    #                         True],\n",
    "    #                        [numpy.asarray(x2 ** (-1.0)).astype(numpy.bool),\n",
    "    #                         numpy.asarray(numpy.sinc(x1 / numpy.pi) ** (-1.0)\n",
    "    #                                       ).astype(numpy.bool)],\n",
    "    #                        default=numpy.nan) + (\n",
    "    #              x1 * abs(x7) + x5) ** 3 + numpy.exp(x7) + numpy.exp(\n",
    "    #             (x1 + x2) / x5) + numpy.sin(numpy.log(x2)))\n",
    "\n",
    "    # X = np.random.randn(1000, 4)\n",
    "    # x1, x2, x3, x4 = X.T\n",
    "    # x1 = np.abs(x1)\n",
    "    # x2 = np.abs(x2)\n",
    "    # y = x1 ** (1 / 4) + np.sqrt(x2) + np.exp(x3 / 2) + np.abs(x4) + np.tan(\n",
    "    #     x4) / x1 ** 2\n",
    "\n",
    "    # X = np.random.randn(1000, 2)\n",
    "    # y = X[:, 0] ** 9 + np.tan(X[:, 1]) + np.abs(X[:, 0] / X[:, 1] ** 2)\n",
    "\n",
    "    # X = np.random.randn(1000, 400)\n",
    "    # y = np.exp(np.random.randn(len(X)))\n",
    "\n",
    "    # X[:, 1] = X[:, 0] / 2\n",
    "    # X[:, 2] = X[:, 1] + 1\n",
    "    # X[:, 3] = X[:, 2] * 2.6\n",
    "    # y = (np.sin(X[:, 0] ** 3) + np.maximum(X[:, 1], 0)\n",
    "    #     - np.sin(X[:, 2]) / X[:, 2] + 2 * X[:, 3])\n",
    "\n",
    "    feature_names = [*range(X.shape[1])]\n",
    "elif dataset_name == 'compas':\n",
    "    dataset_cls = COMPASDataset\n",
    "elif dataset_name == 'heloc':\n",
    "    dataset_cls = HELOCDataset\n",
    "elif dataset_name == 'boston':\n",
    "    dataset_cls = BostonDataset\n",
    "elif dataset_name == 'mnist':\n",
    "    dataset_cls = TinyMNISTDataset\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "    task = 'classification'\n",
    "    # dataset = datasets.load_iris()\n",
    "    # dataset = datasets.load_breast_cancer()\n",
    "    dataset = datasets.load_wine()\n",
    "\n",
    "    X = dataset.data\n",
    "    y = dataset.target\n",
    "\n",
    "# load dataset\n",
    "dataset_orig = dataset_cls()\n",
    "\n",
    "# transform data\n",
    "transformer = Transformer()\n",
    "dataset = transformer.fit_transform(dataset_orig)\n",
    "\n",
    "# extract data\n",
    "task = dataset.task\n",
    "X = dataset.X\n",
    "y = dataset.y\n",
    "feature_names = dataset.feature_names\n",
    "input_shape = dataset.input_shape\n",
    "n_features = dataset.n_features\n",
    "\n",
    "print(f'Samples     = {len(X)}')\n",
    "print(f'Input Shape = {dataset.input_shape}')\n",
    "print(f'Task        = {task}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32125839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "# model_type = 'gam'\n",
    "# model_type = 'dnn'\n",
    "model_type = 'cnn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb6eaadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put together terms\n",
    "max_order = 2\n",
    "min_order = 1\n",
    "\n",
    "if dataset_name == 'heloc':\n",
    "    n_main = n_features - 10\n",
    "    desired_interactions = [(1, 2), (10, 12), (15, 18), (7, 11)]\n",
    "else:\n",
    "    desired_interactions = []\n",
    "    n_main = n_features\n",
    "\n",
    "n_interact = None if desired_interactions else None\n",
    "\n",
    "# current interact plots use this: LIME, MAPLE\n",
    "# desired_interactions = [(1, 2)]\n",
    "\n",
    "# features 8 & 9 correlate in Boston dataset\n",
    "# desired_interactions = [(8, 0, 1), (2, 8), (2, 9)]\n",
    "\n",
    "terms = generate_terms(\n",
    "    n_features=n_features,\n",
    "    n_main=n_main,\n",
    "    n_interact=n_interact,\n",
    "    desired_interactions=desired_interactions,\n",
    "    min_order=min_order,\n",
    "    max_order=max_order,\n",
    "    seed=rng_np,\n",
    ")\n",
    "# terms = [T.te(0, 1), T.te(2, 3), T.s(0, n_splines=50)]\n",
    "# terms = [T.te(0, 1), T.te(1, 3, n_splines=5), T.s(2, n_splines=50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bfea220",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignoring terms\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 21s 21s/step - loss: 7.2492\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7.1464\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7.0671\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7.0042\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 6.9491\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.8963\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.8421\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.7852\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.7260\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.6658\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.6058\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.5469\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.4899\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.4348\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.3813\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 6.3284\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 6.2754\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 6.2220\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 6.1684\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.1149\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.0618\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.0094\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.9576\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.9067\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.8566\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.8072\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.7583\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.7096\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.6609\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.6123\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.5641\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.5166\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 1s 912ms/step - loss: 5.4698\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.4237\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 1s 977ms/step - loss: 5.3782\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.3331\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 1s 781ms/step - loss: 5.2883\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.2437\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.1995\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 1s 874ms/step - loss: 5.1556\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.1123\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 1s 947ms/step - loss: 5.0696\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.0274\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.9856\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.9442\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.9031\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.8623\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.8219\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 4.7820\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 4.7425\n"
     ]
    }
   ],
   "source": [
    "if model_type == 'dnn' or model_type == 'cnn':\n",
    "    callback = EarlyStopping(monitor='loss', mode='min', patience=5,\n",
    "                             restore_best_weights=True)\n",
    "    optimizer = Adam(learning_rate=1e-3)\n",
    "    fit_kwargs = {'epochs': 50, 'batch_size': len(X),\n",
    "                  'callbacks': [callback], 'optimizer': optimizer}\n",
    "else:\n",
    "    fit_kwargs = {}\n",
    "\n",
    "# TODO: factor terms for categoricals in GAM?\n",
    "# TODO: embed categoricals in NN?\n",
    "\n",
    "if model_type == 'dnn':\n",
    "    model = AdditiveDNN(\n",
    "        terms=terms,\n",
    "        task=task,\n",
    "        symbol_names=feature_names,\n",
    "        activation='relu',\n",
    "    )\n",
    "elif model_type == 'cnn':\n",
    "    print('ignoring terms')\n",
    "    model = AdditiveCNN(\n",
    "        task=task,\n",
    "        input_shape=input_shape,\n",
    "        symbol_names=feature_names,\n",
    "        activation='relu',\n",
    "    )\n",
    "elif model_type == 'gam':\n",
    "    if task == 'classification':\n",
    "        model = MultiClassLogisticGAM(symbol_names=feature_names, terms=terms)\n",
    "    else:\n",
    "        model = LinearGAM(symbol_names=feature_names, terms=terms)\n",
    "else:\n",
    "    raise NotImplementedError(model_type)\n",
    "\n",
    "model.fit(X, y, **fit_kwargs)\n",
    "\n",
    "if model_type == 'dnn':\n",
    "    model.plot_model(nonexistent_filename('dnn.png'),\n",
    "                     show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ca10fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_only_this_many = 100\n",
    "# explain_only_this_many = len(X)\n",
    "explain_only_this_many = min(explain_only_this_many, len(X))\n",
    "sample_idxs_all = np.arange(len(X))\n",
    "sample_idxs = rng_np.choice(sample_idxs_all,\n",
    "                            size=explain_only_this_many, replace=False)\n",
    "X_subset = X[sample_idxs]\n",
    "y_subset = y[sample_idxs]\n",
    "\n",
    "true_contribs = model.feature_contributions(X_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4958bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_array = [\n",
    "    # ('SHAPR', SHAPRExplainer),  # >:(\n",
    "    #('VanillaGradients', VanillaGradientsExplainer),\n",
    "    #('VanillaGradients-Smooth', VanillaGradientsExplainer.smooth_grad),\n",
    "    #('GradientsXInputs', GradientsXInputsExplainer),\n",
    "    #('GradientsXInputs-Smooth', GradientsXInputsExplainer.smooth_grad),\n",
    "    #('IntegratedGradients', IntegratedGradientsExplainer),\n",
    "    #('IntegratedGradients-Smooth', IntegratedGradientsExplainer.smooth_grad),\n",
    "    #('Occlusion', OcclusionExplainer),\n",
    "    #('XRAI', XRAIExplainer),\n",
    "    #('XRAI-Smooth', XRAIExplainer.smooth_grad),\n",
    "    #('BlurIG', BlurIntegratedGradientsExplainer),\n",
    "    #('BlurIG-Smooth', BlurIntegratedGradientsExplainer.smooth_grad),\n",
    "    ('LIME', LIMEExplainer),\n",
    "    ('SHAP', KernelSHAPExplainer),\n",
    "]\n",
    "if task == 'regression':\n",
    "    explainer_array.extend([\n",
    "        ('MAPLE', MAPLEExplainer),\n",
    "        ('PDP', PDPExplainer),\n",
    "    ])\n",
    "\n",
    "# TODO: feature_contributions() --> explain()\n",
    "# TODO: explain() --> ExplainerMixin (for both models and explainers)\n",
    "\n",
    "pred_contribs_map = {}\n",
    "pred_y_map = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77e1d588",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining model using SHAP\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 4. Estimator expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f075a03b70b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Explaining model using'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplainer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# fit full dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     pred_contribs, y_pred = explainer.feature_contributions(\n\u001b[1;32m     15\u001b[0m         X_subset, as_dict=True, return_predictions=True)\n",
      "\u001b[0;32m~/Projects/PostHocExplainerEvaluation/posthoceval/explainers/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, grouped_feature_names, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# noinspection PyArgumentList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         self._fit(X=X, y=y, grouped_feature_names=grouped_feature_names,\n\u001b[0;32m---> 74\u001b[0;31m                   **kwargs)\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fitted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/PostHocExplainerEvaluation/posthoceval/explainers/local/shap.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, grouped_feature_names, **fit_kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fitting KernelSHAP'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_explainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpected_value_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_explainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpected_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/PostHocExplainerEvaluation/venv/lib/python3.7/site-packages/alibi/explainers/shap_wrappers.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, background_data, summarise_background, n_background_samples, group_names, groups, weights, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m                     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackground_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mn_background_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKERNEL_SHAP_BACKGROUND_THRESHOLD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0mbackground_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_summarise_background\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackground_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_background_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;31m# check user inputs to provide warnings if input is incorrect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/PostHocExplainerEvaluation/venv/lib/python3.7/site-packages/alibi/explainers/shap_wrappers.py\u001b[0m in \u001b[0;36m_summarise_background\u001b[0;34m(self, background_data, n_background_samples)\u001b[0m\n\u001b[1;32m    536\u001b[0m                 \u001b[0;34m\"through the by passing a weights of len=n_background_samples to the constructor!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m             )\n\u001b[0;32m--> 538\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackground_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_background_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mmethdispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/PostHocExplainerEvaluation/venv/lib/python3.7/site-packages/shap/utils/_legacy.py\u001b[0m in \u001b[0;36mkmeans\u001b[0;34m(X, k, round_values)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# in case there are any missing values in data impute them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mimp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/PostHocExplainerEvaluation/venv/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/PostHocExplainerEvaluation/venv/lib/python3.7/site-packages/sklearn/impute/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \"\"\"\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_fit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;31m# default fill_value is 0 for numerical input and \"missing_value\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/PostHocExplainerEvaluation/venv/lib/python3.7/site-packages/sklearn/impute/_base.py\u001b[0m in \u001b[0;36m_validate_input\u001b[0;34m(self, X, in_fit)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mnew_ve\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0m_check_inputs_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/PostHocExplainerEvaluation/venv/lib/python3.7/site-packages/sklearn/impute/_base.py\u001b[0m in \u001b[0;36m_validate_input\u001b[0;34m(self, X, in_fit)\u001b[0m\n\u001b[1;32m    253\u001b[0m                                     \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                                     \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m                                     copy=self.copy)\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mve\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"could not convert\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mve\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/PostHocExplainerEvaluation/venv/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'no_validation'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/PostHocExplainerEvaluation/venv/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/PostHocExplainerEvaluation/venv/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    658\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n\u001b[0;32m--> 660\u001b[0;31m                              % (array.ndim, estimator_name))\n\u001b[0m\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 4. Estimator expected <= 2."
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['R_HOME'] = '/afs/crc.nd.edu/x86_64_linux/r/R/3.6.2/gcc/4.8.5/bin/R'\n",
    "# del os.environ['R_HOME']\n",
    "import sys\n",
    "sys.path.append('/afs/crc.nd.edu/x86_64_linux/r/R/3.6.2/gcc/4.8.5/bin/')\n",
    "\n",
    "for expl_i, (explainer_name, explainer_cls) in enumerate(explainer_array):\n",
    "    if explainer_name in pred_contribs_map:\n",
    "        print('Skipping', explainer_name)\n",
    "        continue\n",
    "    print('Explaining model using', explainer_name)\n",
    "    explainer = explainer_cls(model, seed=seed, task=task)\n",
    "    explainer.fit(dataset)  # fit full dataset\n",
    "    pred_contribs, y_pred = explainer.feature_contributions(\n",
    "        X_subset, as_dict=True, return_predictions=True)\n",
    "\n",
    "    # store for later viz data generation\n",
    "    pred_contribs_map[explainer_name] = pred_contribs\n",
    "    pred_y_map[explainer_name] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffed508",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: import from viz and implement fully...\n",
    "# plot_fit()\n",
    "\n",
    "df, df_3d, contribs_df, err_dfs = gather_viz_data(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    transformer=transformer,\n",
    "    true_contribs=true_contribs,\n",
    "    pred_contribs_map=pred_contribs_map,\n",
    "    dataset_sample_idxs=sample_idxs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7406671",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col_wrap = 4\n",
    "\n",
    "if df is not None:\n",
    "    # if n_features > 12 or task == 'classification':\n",
    "    #     mpl_qt()\n",
    "    # else:\n",
    "    #     mpl_inline()\n",
    "    g = sns.relplot(\n",
    "        data=df,\n",
    "        x='Feature Value',\n",
    "        y='Contribution',\n",
    "        hue='Explainer',\n",
    "        # col='class' if task == 'classification' else 'true_effect',\n",
    "        col='Class' if task == 'classification' else 'Match',\n",
    "        col_wrap=None if task == 'classification' else col_wrap,\n",
    "        # row='true_effect' if task == 'classification' else None,\n",
    "        row='Match' if task == 'classification' else None,\n",
    "        kind='scatter',\n",
    "        x_jitter=.08,  # for visualization purposes of nearby points\n",
    "        alpha=.65,\n",
    "        facet_kws=dict(sharex=False, sharey=False),\n",
    "    )\n",
    "    for ax in g.axes.flat:\n",
    "        title = ax.get_title()\n",
    "        ax.set_title(title.split(' = ', 1)[1])\n",
    "    g.tight_layout()\n",
    "    g.savefig(nonexistent_filename(f'contributions_grid_{model_type}.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c72fc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3d interaction plot time\n",
    "if df_3d is not None:\n",
    "\n",
    "    plt_x = 'Feature Value x'\n",
    "    plt_y = 'Feature Value y'\n",
    "    plt_z = 'Contribution'\n",
    "    plt_hue = 'Explainer'\n",
    "    plt_col = 'Match'\n",
    "\n",
    "    df_3d_grouped = df_3d.groupby(['Class', plt_col])\n",
    "\n",
    "    n_plots = len(df_3d_grouped)\n",
    "    n_rows = int(np.ceil(n_plots / col_wrap))\n",
    "    n_cols = min(col_wrap, n_plots)\n",
    "    figsize = plt.rcParams['figure.figsize']\n",
    "    figsize = (figsize[0] * n_cols, figsize[1] * n_rows)\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "\n",
    "    for i, ((class_i, ax_title), group_3d) in enumerate(df_3d_grouped):\n",
    "        ax = fig.add_subplot(n_rows, n_cols, i + 1, projection='3d')\n",
    "\n",
    "        for hue_name, hue_df in group_3d.groupby(plt_hue):\n",
    "            ax.scatter(\n",
    "                hue_df[plt_x],\n",
    "                hue_df[plt_y],\n",
    "                hue_df[plt_z],\n",
    "                label=hue_name,\n",
    "                alpha=.5,\n",
    "            )\n",
    "        ax.set_xlabel(plt_x)\n",
    "        ax.set_ylabel(plt_y)\n",
    "        ax.set_zlabel(plt_z)\n",
    "\n",
    "        ax.set_title(ax_title)\n",
    "\n",
    "        if i == 0:\n",
    "            fig.legend(loc='center right')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(nonexistent_filename(\n",
    "        f'contributions_grid_interact_{model_type}.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab376ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_dfs['effectwise_err_agg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16c71da",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_dfs['samplewise_err_agg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a516d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def effects_to_str(*effect_sets):\n",
    "    effect_strs = []\n",
    "    for effects_set in zip(*effect_sets):\n",
    "        features = set()\n",
    "        for effects in effects_set:\n",
    "            for effect in effects:\n",
    "                features.update(effect)\n",
    "        effect_strs.append(' & '.join(map(str, features)))\n",
    "    return effect_strs\n",
    "\n",
    "\n",
    "def plot_explanation(\n",
    "    explanation,\n",
    "):\n",
    "    contribs = contribs_df[(contribs_df['Explainer'] == explanation['Explainer']) &\n",
    "                           (contribs_df['Class'] == explanation['Class'])]\n",
    "    assert len(contribs) == 1\n",
    "    # get contribs for true/pred\n",
    "    contribs = contribs.iloc[0]\n",
    "    true_contribs = contribs['True Contribs']\n",
    "    pred_contribs = contribs['Pred Contribs']\n",
    "    # get attributions for effects\n",
    "    sample_idx = explanation['Sample Index']\n",
    "    # pred\n",
    "    sample_pred_contribs = pred_contribs.iloc[sample_idx]\n",
    "    pred_effects = sample_pred_contribs.keys()\n",
    "    pred_contribs = sample_pred_contribs.values\n",
    "    # true\n",
    "    sample_true_contribs = true_contribs.iloc[sample_idx]\n",
    "    true_effects = sample_true_contribs.keys()\n",
    "    true_contribs = sample_true_contribs.values\n",
    "    # map to readable strings\n",
    "    all_effects = effects_to_str(pred_effects, true_effects)\n",
    "    # set up DF for plotting\n",
    "    df_plot = pd.concat([\n",
    "        pd.DataFrame({\n",
    "            'Effect': all_effects,\n",
    "            'Explainer': 'True',\n",
    "            'Attribution': true_contribs,\n",
    "        }),\n",
    "        pd.DataFrame({\n",
    "            'Effect': all_effects,\n",
    "            'Explainer': explanation['Explainer'],\n",
    "            'Attribution': pred_contribs,\n",
    "        }),\n",
    "    ], ignore_index=True)\n",
    "    f, ax = plt.subplots()\n",
    "    sns.barplot(y='Effect', x='Attribution', hue='Explainer',\n",
    "                data=df_plot, ax=ax,\n",
    "                orient='h')\n",
    "    # ax.set_xscale('symlog')\n",
    "    f.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7995b988",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "worst_by = 'root_mean_squared_error'\n",
    "\n",
    "# Top k worst effects per explainer\n",
    "df_effects = err_dfs['effectwise_err']\n",
    "df_effects = df_effects[df_effects['Metric'] == worst_by]\n",
    "for explainer_name, df_expl_effects in df_effects.groupby(['Explainer']):\n",
    "    df_expl_effects = df_expl_effects.sort_values(by='Score', ascending=False)\n",
    "    print(f'{explainer_name} Top-{k} worst effects:')\n",
    "    display(df_expl_effects.iloc[:k])\n",
    "    print(f'{explainer_name} Top-{k} best effects:')\n",
    "    display(df_expl_effects.iloc[-k:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355f13a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = 3\n",
    "worst_by = 'cosine_distances'\n",
    "# worst_by = 'euclidean_distances'\n",
    "\n",
    "# Top k worst explanations per explainer\n",
    "df_effects = err_dfs['samplewise_err']\n",
    "df_effects = df_effects[df_effects['Metric'] == worst_by]\n",
    "for explainer_name, df_expl_effects in df_effects.groupby(['Explainer']):\n",
    "    df_expl_effects = df_expl_effects.sort_values(by='Score', ascending=False)\n",
    "    print(f'{explainer_name} Top-{k} worst explanations:')\n",
    "    display(df_expl_effects.iloc[:k])\n",
    "    print(f'{explainer_name} Top-{k} best explanations:')\n",
    "    display(df_expl_effects.iloc[-k:])\n",
    "    ax = plot_explanation(df_expl_effects.iloc[0])\n",
    "    ax.set_title(f'{explainer_name} worst explanation')\n",
    "    ax = plot_explanation(df_expl_effects.iloc[-1])\n",
    "    ax.set_title(f'{explainer_name} best explanation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f913db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_model_subset = model.predict(X_subset)\n",
    "if task == 'classification':\n",
    "    y_pred_expl = np.argmax(y_pred_expl, axis=0)\n",
    "    acc = metrics.accuracy(y_subset, y_model_subset)\n",
    "    print(f'Model accuracy={acc * 100:.2f}')\n",
    "else:\n",
    "    err = metrics.rmse(y_subset, y_model_subset)\n",
    "    print(f'Model rmse={err:.3g}')\n",
    "    \n",
    "for explainer_name, y_pred_expl in pred_y_map.items():\n",
    "    if y_pred_expl is None:\n",
    "        print('Skipping', explainer_name, '(no predict() implementation)')\n",
    "        continue\n",
    "    if task == 'classification':\n",
    "        y_pred_expl = np.argmax(y_pred_expl, axis=0)\n",
    "        acc = metrics.accuracy(y_model_subset, y_pred_expl)\n",
    "        print(f'{explainer_name} accuracy={acc * 100:.2f}')\n",
    "    else:\n",
    "        err = metrics.rmse(y_model_subset, y_pred_expl)\n",
    "        print(f'{explainer_name} rmse={err:.3g}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8656c9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14578f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_contribs_map['LIME']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "posthoceval",
   "language": "python",
   "name": "posthoceval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

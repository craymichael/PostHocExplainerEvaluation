{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76cda3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximize reproducibility: set seed with minimal imports\n",
    "# just a seed\n",
    "seed = 431136\n",
    "import os\n",
    "\n",
    "# verbosity\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "# reproducibility\n",
    "# https://github.com/NVIDIA/framework-determinism\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(seed)\n",
    "rng_r = random.Random(seed + 1)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(seed + 2)\n",
    "rng_np = np.random.default_rng(seed + 3)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(seed + 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a8c6c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# from posthoceval.explainers import KernelSHAPExplainer\n",
    "# from posthoceval.explainers import MAPLEExplainer\n",
    "from posthoceval.explainers import *\n",
    "\n",
    "from posthoceval.models.gam import MultiClassLogisticGAM\n",
    "from posthoceval.models.gam import LinearGAM\n",
    "from posthoceval.models.gam import T\n",
    "from posthoceval.models.dnn import AdditiveDNN\n",
    "from posthoceval.transform import Transformer\n",
    "from posthoceval.utils import nonexistent_filename\n",
    "from posthoceval.datasets import COMPASDataset\n",
    "from posthoceval.datasets import BostonDataset\n",
    "from posthoceval.datasets import HELOCDataset\n",
    "from posthoceval.models.term_util import generate_terms\n",
    "from posthoceval.viz import gather_viz_data\n",
    "from posthoceval import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84906cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(\n",
    "    # context='paper',\n",
    "    context='notebook',\n",
    "    style='ticks',\n",
    "    font_scale=1,  # 2.25,\n",
    "    color_codes=True,\n",
    "    # palette=sns.color_palette('pastel'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32852243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpl_backend(backend='inline'):\n",
    "    rc = plt.rcParams.copy()\n",
    "    # backend = plt.get_backend()\n",
    "    %matplotlib $backend\n",
    "    \n",
    "mpl_inline = lambda: mpl_backend('inline')\n",
    "mpl_qt = lambda: mpl_backend('qt')\n",
    "mpl_notebook = lambda: mpl_backend('notebook')\n",
    "mpl_backend('nbagg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00b31a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples     = 6172\n",
      "Input Shape = (19,)\n",
      "Task        = regression\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'compas'\n",
    "\n",
    "if dataset_name == 'synthetic':\n",
    "    raise NotImplementedError\n",
    "    task = 'regression'\n",
    "\n",
    "    # import numpy\n",
    "    # X = np.random.rand(1000, 8) / 4\n",
    "    # x1, x2, x3, x4, x5, x6, x7, x8 = X.T\n",
    "    # y = (x1 ** 2 + x5 ** 2 + x5 * numpy.log(x1 + x2) +\n",
    "    #      x7 * numpy.select([numpy.greater(x2, numpy.sinc(x1 / numpy.pi)),\n",
    "    #                         True],\n",
    "    #                        [numpy.asarray(x2 ** (-1.0)).astype(numpy.bool),\n",
    "    #                         numpy.asarray(numpy.sinc(x1 / numpy.pi) ** (-1.0)\n",
    "    #                                       ).astype(numpy.bool)],\n",
    "    #                        default=numpy.nan) + (\n",
    "    #              x1 * abs(x7) + x5) ** 3 + numpy.exp(x7) + numpy.exp(\n",
    "    #             (x1 + x2) / x5) + numpy.sin(numpy.log(x2)))\n",
    "\n",
    "    # X = np.random.randn(1000, 4)\n",
    "    # x1, x2, x3, x4 = X.T\n",
    "    # x1 = np.abs(x1)\n",
    "    # x2 = np.abs(x2)\n",
    "    # y = x1 ** (1 / 4) + np.sqrt(x2) + np.exp(x3 / 2) + np.abs(x4) + np.tan(\n",
    "    #     x4) / x1 ** 2\n",
    "\n",
    "    # X = np.random.randn(1000, 2)\n",
    "    # y = X[:, 0] ** 9 + np.tan(X[:, 1]) + np.abs(X[:, 0] / X[:, 1] ** 2)\n",
    "\n",
    "    # X = np.random.randn(1000, 400)\n",
    "    # y = np.exp(np.random.randn(len(X)))\n",
    "\n",
    "    # X[:, 1] = X[:, 0] / 2\n",
    "    # X[:, 2] = X[:, 1] + 1\n",
    "    # X[:, 3] = X[:, 2] * 2.6\n",
    "    # y = (np.sin(X[:, 0] ** 3) + np.maximum(X[:, 1], 0)\n",
    "    #     - np.sin(X[:, 2]) / X[:, 2] + 2 * X[:, 3])\n",
    "\n",
    "    feature_names = [*range(X.shape[1])]\n",
    "elif dataset_name == 'compas':\n",
    "    dataset_cls = COMPASDataset\n",
    "elif dataset_name == 'heloc':\n",
    "    dataset_cls = HELOCDataset\n",
    "elif dataset_name == 'boston':\n",
    "    dataset_cls = BostonDataset\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "    task = 'classification'\n",
    "    # dataset = datasets.load_iris()\n",
    "    # dataset = datasets.load_breast_cancer()\n",
    "    dataset = datasets.load_wine()\n",
    "\n",
    "    X = dataset.data\n",
    "    y = dataset.target\n",
    "\n",
    "# load dataset\n",
    "dataset_orig = dataset_cls()\n",
    "\n",
    "# transform data\n",
    "transformer = Transformer()\n",
    "dataset = transformer.fit_transform(dataset_orig)\n",
    "\n",
    "# extract data\n",
    "task = dataset.task\n",
    "X = dataset.X\n",
    "y = dataset.y\n",
    "feature_names = dataset.feature_names\n",
    "n_features = dataset.n_features\n",
    "\n",
    "print(f'Samples     = {len(X)}')\n",
    "print(f'Input Shape = {dataset.input_shape}')\n",
    "print(f'Task        = {task}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32125839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "# model_type = 'gam'\n",
    "model_type = 'dnn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb6eaadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put together terms\n",
    "max_order = 2\n",
    "min_order = 1\n",
    "\n",
    "if dataset_name == 'heloc':\n",
    "    n_main = n_features - 10\n",
    "    desired_interactions = [(1, 2), (10, 12), (15, 18), (7, 11)]\n",
    "else:\n",
    "    desired_interactions = []\n",
    "    n_main = n_features\n",
    "\n",
    "n_interact = None if desired_interactions else None\n",
    "\n",
    "# current interact plots use this: LIME, MAPLE\n",
    "# desired_interactions = [(1, 2)]\n",
    "\n",
    "# features 8 & 9 correlate in Boston dataset\n",
    "# desired_interactions = [(8, 0, 1), (2, 8), (2, 9)]\n",
    "\n",
    "terms = generate_terms(\n",
    "    n_features=n_features,\n",
    "    n_main=n_main,\n",
    "    n_interact=n_interact,\n",
    "    desired_interactions=desired_interactions,\n",
    "    min_order=min_order,\n",
    "    max_order=max_order,\n",
    "    seed=rng_np,\n",
    ")\n",
    "# terms = [T.te(0, 1), T.te(2, 3), T.s(0, n_splines=50)]\n",
    "# terms = [T.te(0, 1), T.te(1, 3, n_splines=5), T.s(2, n_splines=50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bfea220",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 2.0006\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.4520\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.8231\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.1714\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.0557\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.4074\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.4030\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.1132\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.0025\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.1569\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.2648\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.1626\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.0166\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.0132\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "if model_type == 'dnn':\n",
    "    callback = EarlyStopping(monitor='loss', mode='min', patience=5,\n",
    "                             restore_best_weights=True)\n",
    "    optimizer = Adam(learning_rate=1e-3)\n",
    "    fit_kwargs = {'epochs': 50, 'batch_size': len(X),\n",
    "                  'callbacks': [callback], 'optimizer': optimizer}\n",
    "else:\n",
    "    fit_kwargs = {}\n",
    "\n",
    "# TODO: factor terms for categoricals in GAM?\n",
    "# TODO: embed categoricals in NN?\n",
    "\n",
    "if model_type == 'dnn':\n",
    "    model = AdditiveDNN(\n",
    "        terms=terms,\n",
    "        task=task,\n",
    "        symbol_names=feature_names,\n",
    "        activation='sigmoid',\n",
    "    )\n",
    "elif model_type == 'gam':\n",
    "    if task == 'classification':\n",
    "        model = MultiClassLogisticGAM(symbol_names=feature_names, terms=terms)\n",
    "    else:\n",
    "        model = LinearGAM(symbol_names=feature_names, terms=terms)\n",
    "else:\n",
    "    raise NotImplementedError(model_type)\n",
    "\n",
    "model.fit(X, y, **fit_kwargs)\n",
    "\n",
    "if model_type == 'dnn':\n",
    "    model.plot_model(nonexistent_filename('dnn.png'),\n",
    "                     show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ca10fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_only_this_many = 100\n",
    "# explain_only_this_many = len(X)\n",
    "explain_only_this_many = min(explain_only_this_many, len(X))\n",
    "sample_idxs_all = np.arange(len(X))\n",
    "sample_idxs = rng_np.choice(sample_idxs_all,\n",
    "                            size=explain_only_this_many, replace=False)\n",
    "X_subset = X[sample_idxs]\n",
    "y_subset = y[sample_idxs]\n",
    "\n",
    "true_contribs = model.feature_contributions(X_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fcaf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_array = [\n",
    "    # ('SHAPR', SHAPRExplainer),  # >:(\n",
    "    #('VanillaGradients', VanillaGradientsExplainer),\n",
    "    #('VanillaGradients-Smooth', VanillaGradientsExplainer.smooth_grad),\n",
    "    #('GradientsXInputs', GradientsXInputsExplainer),\n",
    "    #('GradientsXInputs-Smooth', GradientsXInputsExplainer.smooth_grad),\n",
    "    #('IntegratedGradients', IntegratedGradientsExplainer),\n",
    "    #('IntegratedGradients-Smooth', IntegratedGradientsExplainer.smooth_grad),\n",
    "    #('Occlusion', OcclusionExplainer),\n",
    "    #('XRAI', XRAIExplainer),\n",
    "    #('XRAI-Smooth', XRAIExplainer.smooth_grad),\n",
    "    #('BlurIG', BlurIntegratedGradientsExplainer),\n",
    "    #('BlurIG-Smooth', BlurIntegratedGradientsExplainer.smooth_grad),\n",
    "    ('LIME', LIMEExplainer),\n",
    "    ('SHAP', KernelSHAPExplainer),\n",
    "]\n",
    "if task == 'regression':\n",
    "    explainer_array.extend([\n",
    "        ('MAPLE', MAPLEExplainer),\n",
    "        ('PDP', PDPExplainer),\n",
    "    ])\n",
    "\n",
    "# TODO: feature_contributions() --> explain()\n",
    "# TODO: explain() --> ExplainerMixin (for both models and explainers)\n",
    "\n",
    "pred_contribs_map = {}\n",
    "pred_y_map = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9164a708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _get_grids(feature_values, num_grid_points, grid_type, percentile_range, grid_range):\n",
    "    \"\"\"Calculate grid points for numeric feature\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    feature_grids: 1d-array\n",
    "        calculated grid points\n",
    "    percentile_info: 1d-array or []\n",
    "        percentile information for feature_grids\n",
    "        exists when grid_type='percentile'\n",
    "    \"\"\"\n",
    "\n",
    "    if grid_type == 'percentile':\n",
    "        # grid points are calculated based on percentile in unique level\n",
    "        # thus the final number of grid points might be smaller than num_grid_points\n",
    "        start, end = 0, 100\n",
    "        if percentile_range is not None:\n",
    "            start, end = np.min(percentile_range), np.max(percentile_range)\n",
    "\n",
    "        percentile_grids = np.linspace(start=start, stop=end, num=num_grid_points)\n",
    "        value_grids = np.percentile(feature_values, percentile_grids)\n",
    "\n",
    "        grids_df = pd.DataFrame()\n",
    "        grids_df['percentile_grids'] = [round(v, 2) for v in percentile_grids]\n",
    "        grids_df['value_grids'] = value_grids\n",
    "        grids_df = grids_df.groupby(['value_grids'], as_index=False).agg(\n",
    "            {'percentile_grids': lambda v: str(tuple(v)).replace(',)', ')')}).sort_values('value_grids', ascending=True)\n",
    "\n",
    "        feature_grids, percentile_info = grids_df['value_grids'].values, grids_df['percentile_grids'].values\n",
    "        \n",
    "    return feature_grids, percentile_info\n",
    "\n",
    "\n",
    "feature_names__ = model.symbol_names\n",
    "dataset__ = pd.DataFrame(\n",
    "            columns=feature_names__,\n",
    "            data=dataset.X,\n",
    "        )\n",
    "_get_grids(\n",
    "                feature_values=dataset__[feature_names__[1]].values, num_grid_points=100, grid_type='percentile',\n",
    "                percentile_range=None, grid_range=None)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "77e1d588",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping LIME\n",
      "Skipping SHAP\n",
      "Skipping MAPLE\n",
      "Explaining model using PDP\n",
      "die\n",
      "(6172, 19)\n",
      "(6172,)\n",
      "Age\n",
      "(46,)\n",
      "(46,)\n",
      "Juvenile Crime Count (Felony)\n",
      "(4,)\n",
      "(4,)\n",
      "Juvenile Crime Count (Misdemeanor)\n",
      "(4,)\n",
      "(4,)\n",
      "Juvenile Crime Count (Other)\n",
      "(4,)\n",
      "(4,)\n",
      "Priors Count\n",
      "(19,)\n",
      "(19,)\n",
      "['Sex = Female', 'Sex = Male']\n",
      "(2,)\n",
      "(2,)\n",
      "['Race = African-American', 'Race = Asian', 'Race = Caucasian', 'Race = Hispanic', 'Race = Native American', 'Race = Other']\n",
      "(6,)\n",
      "(6,)\n",
      "['Charge Degree = F', 'Charge Degree = M']\n",
      "(2,)\n",
      "(2,)\n",
      "['Recidivated = 0', 'Recidivated = 1']\n",
      "(2,)\n",
      "(2,)\n",
      "['Recidivated in Two Years = 0', 'Recidivated in Two Years = 1']\n",
      "(2,)\n",
      "(2,)\n",
      "(6172, 19)\n",
      "19\n",
      "(10,)\n",
      "(10,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected matrix of rank 2 but received vector of rank 1 instead for y.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-f075a03b70b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Explaining model using'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplainer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# fit full dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     pred_contribs, y_pred = explainer.feature_contributions(\n\u001b[1;32m     15\u001b[0m         X_subset, as_dict=True, return_predictions=True)\n",
      "\u001b[0;32m~/Projects/PostHocExplainerEvaluation/posthoceval/explainers/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, grouped_feature_names, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# noinspection PyArgumentList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         self._fit(X=X, y=y, grouped_feature_names=grouped_feature_names,\n\u001b[0;32m---> 74\u001b[0;31m                   **kwargs)\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fitted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/PostHocExplainerEvaluation/posthoceval/explainers/global_/pdp.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, grouped_feature_names)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             )\n\u001b[1;32m    119\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/PostHocExplainerEvaluation/posthoceval/explainers/global_/global_util.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, interpolation)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \"\"\"\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# n data point, k features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_same_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/PostHocExplainerEvaluation/posthoceval/utils.py\u001b[0m in \u001b[0;36massert_rank\u001b[0;34m(a, rank, name)\u001b[0m\n\u001b[1;32m     99\u001b[0m         raise ValueError('Expected %s of rank %d but received %s of rank %d '\n\u001b[1;32m    100\u001b[0m                          'instead%s.' % (exp_type, rank, act_type, actual_rank,\n\u001b[0;32m--> 101\u001b[0;31m                                          ' for ' + name if name else ''))\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected matrix of rank 2 but received vector of rank 1 instead for y."
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['R_HOME'] = '/afs/crc.nd.edu/x86_64_linux/r/R/3.6.2/gcc/4.8.5/bin/R'\n",
    "# del os.environ['R_HOME']\n",
    "import sys\n",
    "sys.path.append('/afs/crc.nd.edu/x86_64_linux/r/R/3.6.2/gcc/4.8.5/bin/')\n",
    "\n",
    "for expl_i, (explainer_name, explainer_cls) in enumerate(explainer_array):\n",
    "    if explainer_name in pred_contribs_map:\n",
    "        print('Skipping', explainer_name)\n",
    "        continue\n",
    "    print('Explaining model using', explainer_name)\n",
    "    explainer = explainer_cls(model, seed=seed, task=task)\n",
    "    explainer.fit(dataset)  # fit full dataset\n",
    "    pred_contribs, y_pred = explainer.feature_contributions(\n",
    "        X_subset, as_dict=True, return_predictions=True)\n",
    "\n",
    "    # store for later viz data generation\n",
    "    pred_contribs_map[explainer_name] = pred_contribs\n",
    "    pred_y_map[explainer_name] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffed508",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: import from viz and implement fully...\n",
    "# plot_fit()\n",
    "\n",
    "df, df_3d, contribs_df, err_dfs = gather_viz_data(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    transformer=transformer,\n",
    "    true_contribs=true_contribs,\n",
    "    pred_contribs_map=pred_contribs_map,\n",
    "    dataset_sample_idxs=sample_idxs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7406671",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col_wrap = 4\n",
    "\n",
    "if df is not None:\n",
    "    # if n_features > 12 or task == 'classification':\n",
    "    #     mpl_qt()\n",
    "    # else:\n",
    "    #     mpl_inline()\n",
    "    g = sns.relplot(\n",
    "        data=df,\n",
    "        x='Feature Value',\n",
    "        y='Contribution',\n",
    "        hue='Explainer',\n",
    "        # col='class' if task == 'classification' else 'true_effect',\n",
    "        col='Class' if task == 'classification' else 'Match',\n",
    "        col_wrap=None if task == 'classification' else col_wrap,\n",
    "        # row='true_effect' if task == 'classification' else None,\n",
    "        row='Match' if task == 'classification' else None,\n",
    "        kind='scatter',\n",
    "        x_jitter=.08,  # for visualization purposes of nearby points\n",
    "        alpha=.65,\n",
    "        facet_kws=dict(sharex=False, sharey=False),\n",
    "    )\n",
    "    for ax in g.axes.flat:\n",
    "        title = ax.get_title()\n",
    "        ax.set_title(title.split(' = ', 1)[1])\n",
    "    g.tight_layout()\n",
    "    g.savefig(nonexistent_filename(f'contributions_grid_{model_type}.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c72fc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3d interaction plot time\n",
    "if df_3d is not None:\n",
    "\n",
    "    plt_x = 'Feature Value x'\n",
    "    plt_y = 'Feature Value y'\n",
    "    plt_z = 'Contribution'\n",
    "    plt_hue = 'Explainer'\n",
    "    plt_col = 'Match'\n",
    "\n",
    "    df_3d_grouped = df_3d.groupby(['Class', plt_col])\n",
    "\n",
    "    n_plots = len(df_3d_grouped)\n",
    "    n_rows = int(np.ceil(n_plots / col_wrap))\n",
    "    n_cols = min(col_wrap, n_plots)\n",
    "    figsize = plt.rcParams['figure.figsize']\n",
    "    figsize = (figsize[0] * n_cols, figsize[1] * n_rows)\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "\n",
    "    for i, ((class_i, ax_title), group_3d) in enumerate(df_3d_grouped):\n",
    "        ax = fig.add_subplot(n_rows, n_cols, i + 1, projection='3d')\n",
    "\n",
    "        for hue_name, hue_df in group_3d.groupby(plt_hue):\n",
    "            ax.scatter(\n",
    "                hue_df[plt_x],\n",
    "                hue_df[plt_y],\n",
    "                hue_df[plt_z],\n",
    "                label=hue_name,\n",
    "                alpha=.5,\n",
    "            )\n",
    "        ax.set_xlabel(plt_x)\n",
    "        ax.set_ylabel(plt_y)\n",
    "        ax.set_zlabel(plt_z)\n",
    "\n",
    "        ax.set_title(ax_title)\n",
    "\n",
    "        if i == 0:\n",
    "            fig.legend(loc='center right')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(nonexistent_filename(\n",
    "        f'contributions_grid_interact_{model_type}.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab376ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_dfs['effectwise_err_agg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16c71da",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_dfs['samplewise_err_agg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d5dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def effects_to_str(*effect_sets):\n",
    "    effect_strs = []\n",
    "    for effects_set in zip(*effect_sets):\n",
    "        features = set()\n",
    "        for effects in effects_set:\n",
    "            for effect in effects:\n",
    "                features.update(effect)\n",
    "        effect_strs.append(' & '.join(map(str, features)))\n",
    "    return effect_strs\n",
    "\n",
    "\n",
    "def plot_explanation(\n",
    "    explanation,\n",
    "):\n",
    "    contribs = contribs_df[(contribs_df['Explainer'] == explanation['Explainer']) &\n",
    "                           (contribs_df['Class'] == explanation['Class'])]\n",
    "    assert len(contribs) == 1\n",
    "    # get contribs for true/pred\n",
    "    contribs = contribs.iloc[0]\n",
    "    true_contribs = contribs['True Contribs']\n",
    "    pred_contribs = contribs['Pred Contribs']\n",
    "    # get attributions for effects\n",
    "    sample_idx = explanation['Sample Index']\n",
    "    # pred\n",
    "    sample_pred_contribs = pred_contribs.iloc[sample_idx]\n",
    "    pred_effects = sample_pred_contribs.keys()\n",
    "    pred_contribs = sample_pred_contribs.values\n",
    "    # true\n",
    "    sample_true_contribs = true_contribs.iloc[sample_idx]\n",
    "    true_effects = sample_true_contribs.keys()\n",
    "    true_contribs = sample_true_contribs.values\n",
    "    # map to readable strings\n",
    "    all_effects = effects_to_str(pred_effects, true_effects)\n",
    "    # set up DF for plotting\n",
    "    df_plot = pd.concat([\n",
    "        pd.DataFrame({\n",
    "            'Effect': all_effects,\n",
    "            'Explainer': 'True',\n",
    "            'Attribution': true_contribs,\n",
    "        }),\n",
    "        pd.DataFrame({\n",
    "            'Effect': all_effects,\n",
    "            'Explainer': explanation['Explainer'],\n",
    "            'Attribution': pred_contribs,\n",
    "        }),\n",
    "    ], ignore_index=True)\n",
    "    f, ax = plt.subplots()\n",
    "    sns.barplot(y='Effect', x='Attribution', hue='Explainer',\n",
    "                data=df_plot, ax=ax,\n",
    "                orient='h')\n",
    "    # ax.set_xscale('symlog')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7995b988",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "worst_by = 'root_mean_squared_error'\n",
    "\n",
    "# Top k worst effects per explainer\n",
    "df_effects = err_dfs['effectwise_err']\n",
    "df_effects = df_effects[df_effects['Metric'] == worst_by]\n",
    "for explainer_name, df_expl_effects in df_effects.groupby(['Explainer']):\n",
    "    df_expl_effects = df_expl_effects.sort_values(by='Score', ascending=False)\n",
    "    print(f'{explainer_name} Top-{k} worst effects:')\n",
    "    display(df_expl_effects.iloc[:k])\n",
    "    print(f'{explainer_name} Top-{k} best effects:')\n",
    "    display(df_expl_effects.iloc[-k:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355f13a7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "k = 3\n",
    "worst_by = 'cosine_distances'\n",
    "# worst_by = 'euclidean_distances'\n",
    "\n",
    "# Top k worst explanations per explainer\n",
    "df_effects = err_dfs['samplewise_err']\n",
    "df_effects = df_effects[df_effects['Metric'] == worst_by]\n",
    "for explainer_name, df_expl_effects in df_effects.groupby(['Explainer']):\n",
    "    df_expl_effects = df_expl_effects.sort_values(by='Score', ascending=False)\n",
    "    print(f'{explainer_name} Top-{k} worst explanations:')\n",
    "    display(df_expl_effects.iloc[:k])\n",
    "    print(f'{explainer_name} Top-{k} best explanations:')\n",
    "    display(df_expl_effects.iloc[-k:])\n",
    "    ax = plot_explanation(df_expl_effects.iloc[0])\n",
    "    ax.set_title(f'{explainer_name} worst explanation')\n",
    "    ax = plot_explanation(df_expl_effects.iloc[-1])\n",
    "    ax.set_title(f'{explainer_name} best explanation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375eeaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_model_subset = model.predict(X_subset)\n",
    "if task == 'classification':\n",
    "    y_pred_expl = np.argmax(y_pred_expl, axis=0)\n",
    "    acc = metrics.accuracy(y_subset, y_model_subset)\n",
    "    print(f'Model accuracy={acc * 100:.2f}')\n",
    "else:\n",
    "    err = metrics.rmse(y_subset, y_model_subset)\n",
    "    print(f'Model rmse={err:.3g}')\n",
    "    \n",
    "for explainer_name, y_pred_expl in pred_y_map.items():\n",
    "    if task == 'classification':\n",
    "        y_pred_expl = np.argmax(y_pred_expl, axis=0)\n",
    "        acc = metrics.accuracy(y_model_subset, y_pred_expl)\n",
    "        print(f'{explainer_name} accuracy={acc * 100:.2f}')\n",
    "    else:\n",
    "        err = metrics.rmse(y_model_subset, y_pred_expl)\n",
    "        print(f'{explainer_name} rmse={err:.3g}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "posthoceval",
   "language": "python",
   "name": "posthoceval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

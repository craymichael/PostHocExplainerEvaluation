{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76cda3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximize reproducibility: set seed with minimal imports\n",
    "# just a seed\n",
    "seed = 431136\n",
    "import os\n",
    "\n",
    "# verbosity\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "# reproducibility\n",
    "# https://github.com/NVIDIA/framework-determinism\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(seed)\n",
    "rng_r = random.Random(seed + 1)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(seed + 2)\n",
    "rng_np = np.random.default_rng(seed + 3)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(seed + 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a8c6c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from posthoceval.explainers import KernelSHAPExplainer\n",
    "from posthoceval.models.gam import MultiClassLogisticGAM\n",
    "from posthoceval.models.gam import LinearGAM\n",
    "from posthoceval.models.gam import T\n",
    "from posthoceval.models.dnn import AdditiveDNN\n",
    "from posthoceval.transform import Transformer\n",
    "from posthoceval.utils import nonexistent_filename\n",
    "from posthoceval.datasets import COMPASDataset\n",
    "from posthoceval.datasets import BostonDataset\n",
    "from posthoceval.models.term_util import generate_terms\n",
    "from posthoceval.viz import gather_viz_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84906cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(\n",
    "    context='paper',\n",
    "    # context='notebook',\n",
    "    style='ticks',\n",
    "    font_scale=2.25,\n",
    "    color_codes=True,\n",
    "    # palette=sns.color_palette('pastel'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00b31a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'compas'\n",
    "\n",
    "if dataset_name == 'synthetic':\n",
    "    raise NotImplementedError\n",
    "    task = 'regression'\n",
    "\n",
    "    # import numpy\n",
    "    # X = np.random.rand(1000, 8) / 4\n",
    "    # x1, x2, x3, x4, x5, x6, x7, x8 = X.T\n",
    "    # y = (x1 ** 2 + x5 ** 2 + x5 * numpy.log(x1 + x2) +\n",
    "    #      x7 * numpy.select([numpy.greater(x2, numpy.sinc(x1 / numpy.pi)),\n",
    "    #                         True],\n",
    "    #                        [numpy.asarray(x2 ** (-1.0)).astype(numpy.bool),\n",
    "    #                         numpy.asarray(numpy.sinc(x1 / numpy.pi) ** (-1.0)\n",
    "    #                                       ).astype(numpy.bool)],\n",
    "    #                        default=numpy.nan) + (\n",
    "    #              x1 * abs(x7) + x5) ** 3 + numpy.exp(x7) + numpy.exp(\n",
    "    #             (x1 + x2) / x5) + numpy.sin(numpy.log(x2)))\n",
    "\n",
    "    # X = np.random.randn(1000, 4)\n",
    "    # x1, x2, x3, x4 = X.T\n",
    "    # x1 = np.abs(x1)\n",
    "    # x2 = np.abs(x2)\n",
    "    # y = x1 ** (1 / 4) + np.sqrt(x2) + np.exp(x3 / 2) + np.abs(x4) + np.tan(\n",
    "    #     x4) / x1 ** 2\n",
    "\n",
    "    # X = np.random.randn(1000, 2)\n",
    "    # y = X[:, 0] ** 9 + np.tan(X[:, 1]) + np.abs(X[:, 0] / X[:, 1] ** 2)\n",
    "\n",
    "    # X = np.random.randn(1000, 400)\n",
    "    # y = np.exp(np.random.randn(len(X)))\n",
    "\n",
    "    # X[:, 1] = X[:, 0] / 2\n",
    "    # X[:, 2] = X[:, 1] + 1\n",
    "    # X[:, 3] = X[:, 2] * 2.6\n",
    "    # y = (np.sin(X[:, 0] ** 3) + np.maximum(X[:, 1], 0)\n",
    "    #     - np.sin(X[:, 2]) / X[:, 2] + 2 * X[:, 3])\n",
    "\n",
    "    feature_names = [*range(X.shape[1])]\n",
    "elif dataset_name == 'compas':\n",
    "    dataset_cls = COMPASDataset\n",
    "elif dataset_name == 'boston':\n",
    "    dataset_cls = BostonDataset\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "    task = 'classification'\n",
    "    # dataset = datasets.load_iris()\n",
    "    # dataset = datasets.load_breast_cancer()\n",
    "    dataset = datasets.load_wine()\n",
    "\n",
    "    X = dataset.data\n",
    "    y = dataset.target\n",
    "\n",
    "# load dataset\n",
    "dataset_orig = dataset_cls()\n",
    "\n",
    "# transform data\n",
    "transformer = Transformer()\n",
    "dataset = transformer.fit_transform(dataset_orig)\n",
    "\n",
    "# extract data\n",
    "task = dataset.task\n",
    "X = dataset.X\n",
    "y = dataset.y\n",
    "feature_names = dataset.feature_names\n",
    "n_features = dataset.n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32125839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "# model_type = 'gam'\n",
    "model_type = 'dnn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb6eaadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put together terms\n",
    "desired_interactions = []\n",
    "\n",
    "# current interact plots use this: LIME, MAPLE\n",
    "# desired_interactions = [(1, 2)]\n",
    "\n",
    "# features 8 & 9 correlate in Boston dataset\n",
    "# desired_interactions = [(8, 0, 1), (2, 8), (2, 9)]\n",
    "# desired_interactions = [(5, 8), (5, 9)]\n",
    "# desired_interactions = [(2, 8), (5, 9)]\n",
    "\n",
    "# desired_interactions = [(1, 2), (4, 9), (8, 10)]\n",
    "\n",
    "# desired_interactions = [(0, 1), (2, 3), (4, 5), (6, 7), (8, 9), (10, 11, 12)]\n",
    "# desired_interactions = [(0, 1, 2, 3), (4, 5), (6, 7), (8, 9), (10, 11, 12)]\n",
    "# desired_interactions = [(0, 1, 2, 3), (4, 5), (6, 7), (8, 9), (10, 11, 12),\n",
    "#                         (220, 101)]\n",
    "\n",
    "max_order = 2\n",
    "min_order = 1\n",
    "n_main = n_features\n",
    "n_interact = len(desired_interactions) if desired_interactions else None\n",
    "\n",
    "terms = generate_terms(\n",
    "    n_features=n_features,\n",
    "    n_main=n_main,\n",
    "    n_interact=n_interact,\n",
    "    desired_interactions=desired_interactions,\n",
    "    min_order=min_order,\n",
    "    max_order=max_order,\n",
    "    seed=rng_np,\n",
    ")\n",
    "# terms = [T.te(0, 1), T.te(2, 3), T.s(0, n_splines=50)]\n",
    "# terms = [T.te(0, 1), T.te(1, 3, n_splines=5), T.s(2, n_splines=50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bfea220",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.8974\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.7350\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6482\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6037\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5958\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6018\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6023\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5974\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5879\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5751\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "if model_type == 'dnn':\n",
    "    callback = EarlyStopping(monitor='loss', mode='min', patience=100,\n",
    "                             restore_best_weights=True)\n",
    "    optimizer = Adam(learning_rate=1e-3)\n",
    "    fit_kwargs = {'epochs': 10, 'batch_size': len(X),\n",
    "                  'callbacks': [callback], 'optimizer': optimizer}\n",
    "else:\n",
    "    fit_kwargs = {}\n",
    "\n",
    "# TODO: factor terms for categoricals in GAM?\n",
    "# TODO: embed categoricals in NN?\n",
    "\n",
    "if model_type == 'dnn':\n",
    "    model = AdditiveDNN(\n",
    "        terms=terms,\n",
    "        task=task,\n",
    "        symbol_names=feature_names,\n",
    "    )\n",
    "elif model_type == 'gam':\n",
    "    if task == 'classification':\n",
    "        model = MultiClassLogisticGAM(symbol_names=feature_names, terms=terms)\n",
    "    else:\n",
    "        model = LinearGAM(symbol_names=feature_names, terms=terms)\n",
    "else:\n",
    "    raise NotImplementedError(model_type)\n",
    "\n",
    "model.fit(X, y, **fit_kwargs)\n",
    "\n",
    "if model_type == 'dnn':\n",
    "    model.plot_model(nonexistent_filename('dnn.png'),\n",
    "                     show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ca10fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_only_this_many = 2\n",
    "# explain_only_this_many = 512\n",
    "# explain_only_this_many = 101\n",
    "# explain_only_this_many = 12\n",
    "# explain_only_this_many = len(X)\n",
    "explain_only_this_many = min(explain_only_this_many, len(X))\n",
    "sample_idxs_all = np.arange(len(X))\n",
    "sample_idxs = rng_np.choice(sample_idxs_all,\n",
    "                            size=explain_only_this_many, replace=False)\n",
    "X_subset = X[sample_idxs]\n",
    "\n",
    "true_contribs = model.feature_contributions(X_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77e1d588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining model using SHAP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predictor returned a scalar value. Ensure the output represents a probability or decision score as opposed to a classification label!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69f56118daa4c5588084705fa0ac4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explainer_array = (\n",
    "    # ('LIME',\n",
    "    #  LIMEExplainer(model, seed=seed, task=task)),\n",
    "    # ('MAPLE',\n",
    "    #  MAPLEExplainer(model, seed=seed, task=task)),\n",
    "    ('SHAP',\n",
    "     KernelSHAPExplainer(model, task=task, seed=seed,\n",
    "                         n_cpus=1 if model_type == 'dnn' else -1)),\n",
    ")\n",
    "\n",
    "# TODO: feature_contributions() --> explain()\n",
    "# TODO: explain() --> ExplainerMixin (for both models and explainers)\n",
    "\n",
    "pred_contribs_map = {}\n",
    "pred_y_map = {}\n",
    "for expl_i, (explainer_name, explainer) in enumerate(explainer_array):\n",
    "    print('Explaining model using', explainer_name)\n",
    "    explainer.fit(dataset)  # fit full dataset\n",
    "    pred_contribs, y_pred = explainer.feature_contributions(\n",
    "        X_subset, as_dict=True, return_predictions=True)\n",
    "\n",
    "    # store for later viz data generation\n",
    "    pred_contribs_map[explainer_name] = pred_contribs\n",
    "    pred_y_map[explainer_name] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1474b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ffed508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP Race = Asian| mean_squared_error=0.000 | normalized_root_mean_squared_error=0.000 | mean_absolute_percentage_error=0.000 | pearson_correlation_coef=nan | spearman_rank_correlation=nan\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data must be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-f28382ddd9ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpred_contribs_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred_contribs_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdataset_sample_idxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_idxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0merr_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;32m~/Projects/PostHocExplainerEvaluation/posthoceval/viz.py\u001b[0m in \u001b[0;36mgather_viz_data\u001b[0;34m(model, dataset, transformer, true_contribs, pred_contribs_map, dataset_sample_idxs, err_func)\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0merr_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mdataset_sample_idxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_sample_idxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0mignored_true_effects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignored_true_effects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m             )\n\u001b[1;32m    107\u001b[0m             \u001b[0mdfs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdfs_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/PostHocExplainerEvaluation/posthoceval/viz.py\u001b[0m in \u001b[0;36m_gather_viz_data_single_output\u001b[0;34m(true_contribs_k, pred_contribs_k, dataset, transformer, model, explainer_name, target_str, err_func, dataset_sample_idxs, ignored_true_effects)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mpred_df_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Explainer'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0mpred_df_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Contribution'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_contrib_i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0mstore_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_df_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;31m# true contributions (model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mignore_true_effect\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mno_true_contrib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/PostHocExplainerEvaluation/venv/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/PostHocExplainerEvaluation/venv/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         ]\n\u001b[0;32m--> 287\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/PostHocExplainerEvaluation/venv/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# don't force copy because getting jammed in an ndarray anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_homogenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/PostHocExplainerEvaluation/venv/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_homogenize\u001b[0;34m(data, index, dtype)\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_multiget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             val = sanitize_array(\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m             )\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/PostHocExplainerEvaluation/venv/lib/python3.7/site-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0msubarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data must be 1-dimensional\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray_tuplesafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Data must be 1-dimensional"
     ]
    }
   ],
   "source": [
    "# TODO: import from viz and implement fully...\n",
    "# plot_fit()\n",
    "\n",
    "df, df_3d = gather_viz_data(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    transformer=transformer,\n",
    "    true_contribs=true_contribs,\n",
    "    pred_contribs_map=pred_contribs_map,\n",
    "    dataset_sample_idxs=sample_idxs,\n",
    "    err_func=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7406671",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_wrap = 4\n",
    "\n",
    "if not df.empty:\n",
    "    g = sns.relplot(\n",
    "        data=df,\n",
    "        x='Feature Value',\n",
    "        y='Contribution',\n",
    "        hue='Explainer',\n",
    "        # col='class' if task == 'classification' else 'true_effect',\n",
    "        col='Class' if task == 'classification' else 'Match',\n",
    "        col_wrap=None if task == 'classification' else col_wrap,\n",
    "        # row='true_effect' if task == 'classification' else None,\n",
    "        row='Match' if task == 'classification' else None,\n",
    "        kind='scatter',\n",
    "        x_jitter=.08,  # for visualization purposes of nearby points\n",
    "        alpha=.65,\n",
    "        facet_kws=dict(sharex=False, sharey=False),\n",
    "    )\n",
    "    for ax in g.axes.flat:\n",
    "        title = ax.get_title()\n",
    "        ax.set_title(title.split(' = ', 1)[1])\n",
    "    g.savefig(nonexistent_filename(f'contributions_grid_{model_type}.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c72fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3d interaction plot time\n",
    "if not df_3d.empty:\n",
    "\n",
    "    plt_x = 'Feature Value x'\n",
    "    plt_y = 'Feature Value y'\n",
    "    plt_z = 'Contribution'\n",
    "    plt_hue = 'Explainer'\n",
    "    plt_col = 'Match'\n",
    "\n",
    "    df_3d_grouped = df_3d.groupby(['Class', plt_col])\n",
    "\n",
    "    n_plots = len(df_3d_grouped)\n",
    "    n_rows = int(np.ceil(n_plots / col_wrap))\n",
    "    n_cols = min(col_wrap, n_plots)\n",
    "    figsize = plt.rcParams['figure.figsize']\n",
    "    figsize = (figsize[0] * n_cols, figsize[1] * n_rows)\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "\n",
    "    for i, ((class_i, ax_title), group_3d) in enumerate(df_3d_grouped):\n",
    "        ax = fig.add_subplot(n_rows, n_cols, i + 1, projection='3d')\n",
    "\n",
    "        for hue_name, hue_df in group_3d.groupby(plt_hue):\n",
    "            ax.scatter(\n",
    "                hue_df[plt_x],\n",
    "                hue_df[plt_y],\n",
    "                hue_df[plt_z],\n",
    "                label=hue_name,\n",
    "                alpha=.5,\n",
    "            )\n",
    "        ax.set_xlabel(plt_x)\n",
    "        ax.set_ylabel(plt_y)\n",
    "        ax.set_zlabel(plt_z)\n",
    "\n",
    "        ax.set_title(ax_title)\n",
    "\n",
    "        if i == 0:\n",
    "            fig.legend(loc='center right')\n",
    "    fig.savefig(nonexistent_filename(\n",
    "        f'contributions_grid_interact_{model_type}.pdf'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "posthoceval",
   "language": "python",
   "name": "posthoceval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
